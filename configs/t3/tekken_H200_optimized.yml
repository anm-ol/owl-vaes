# In configs/t3/tekken_H200_optimized.yml
#for training : torchrun --nproc_per_node=8 -m train --config_path configs/t3/tekken_H200_optimized.yml
model:
  model_id: dcae
  sample_size: [448, 736] 
  channels: 4 # RGB + Pose 
  latent_size: 8
  latent_channels: 128
  ch_0: 256
  ch_max: 2048
  encoder_blocks_per_stage: [4, 4, 4, 4, 4]
  decoder_blocks_per_stage: [4, 4, 4, 4, 4]
  use_middle_block: false

train:
  trainer_id: rec
  data_id: t3_live_merge_4channel
  data_kwargs:
    # <-- IMPORTANT: Make sure these paths are correct for your system
    root: "t3_data/"
    pose_root: "t3_pose/"
    target_size: [448, 736]
    pose_suffix: "_two_player_poses.npz" 

  # <-- OPTIMIZED: Massively increased batch size for H200s
  target_batch_size: 128 # Effective global batch size (128 * 2 GPUs)
  batch_size: 16  # Per-GPU batch size. You can likely increase this even more.

  epochs: 500

  opt: AdamW
  opt_kwargs:
    lr: 3.0e-5
    weight_decay: 1.0e-4
    betas: [0.9, 0.95]
    eps: 1.0e-15

  lpips_type: convnext
  loss_weights:
    kl: 1.0e-6
    lpips: 12.0
    l2: 1.0
    l1_pose: 0 #added l1 loss for pose, initially set 1 but failed to reconstruct 
  scheduler: LinearWarmup
  scheduler_kwargs:
    warmup_steps: 2000
    min_lr: 3.0e-6

  checkpoint_dir: checkpoints/tekken_vae_H200_v6
<<<<<<< HEAD
<<<<<<< HEAD
  resume_ckpt: checkpoints/tekken_vae_H200_v6/step_30000.pt #null
=======
  resume_ckpt: checkpoints/tekken_vae_H200_v6/step_10000.pt #null
>>>>>>> 0cd2721873ce40926f16cce6e1cc5f50b0bfefa2
=======
  resume_ckpt: checkpoints/tekken_vae_H200_v6/step_10000.pt #null
>>>>>>> 0cd2721873ce40926f16cce6e1cc5f50b0bfefa2

  sample_interval: 400
  save_interval: 2000
wandb:
  name: ${env:praymesh}
  project: t3VAE
  # A more descriptive run name for the new hardware
  run_name: tekken_vae_H200_v6
